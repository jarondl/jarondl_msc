%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[onecolumn,fleqn,notitlepage,secnumarabic]{revtex4}

% special 
\usepackage{ifthen}
\usepackage{ifpdf}
\usepackage{float}
\usepackage{color}

% fonts
\usepackage{latexsym}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{bm}
\usepackage{wasysym}


\ifpdf
\usepackage{graphicx}
\usepackage{epstopdf}
\else
\usepackage{graphicx}
\usepackage{epsfig}
\fi

%by jarondl
\usepackage{subfig}
\usepackage{verbatim} % for multiline comments
\usepackage{natbib} 
\usepackage{fancybox}
\usepackage{cmap}  % for making pdf mathmode searchable
%\usepackage{sectsty}
\usepackage[pdftitle={Notes by Jarondl}]{hyperref}  % for hyperlinks in biblio. should be called last?

\graphicspath{{figures/},{PROG/figures/}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% NEW 
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\varphiJ}{\bm{\varphi}}
\newcommand{\thetaJ}{\bm{\theta}}
%\renewcommand{\includegraphics}[2][0]{figure}[h!]


% math symbols I
\newcommand{\sinc}{\mbox{sinc}}
\newcommand{\const}{\mbox{const}}
\newcommand{\trc}{\mbox{trace}}
\newcommand{\intt}{\int\!\!\!\!\int }
\newcommand{\ointt}{\int\!\!\!\!\int\!\!\!\!\!\circ\ }
\newcommand{\ar}{\mathsf r}
\newcommand{\im}{\mbox{Im}}
\newcommand{\re}{\mbox{Re}}

% math symbols II
\newcommand{\eexp}{\mbox{e}^}
\newcommand{\bra}{\left\langle}
\newcommand{\ket}{\right\rangle}

% Mass symbol
\newcommand{\mass}{\mathsf{m}} 
\newcommand{\rdisc}{\epsilon} 

% more math commands
\newcommand{\tbox}[1]{\mbox{\tiny #1}}
\newcommand{\bmsf}[1]{\bm{\mathsf{#1}}} 
\newcommand{\amatrix}[1]{\begin{matrix} #1 \end{matrix}} 
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% equations
\newcommand{\be}[1]{\begin{eqnarray}\ifthenelse{#1=-1}{\nonumber}{\ifthenelse{#1=0}{}{\label{e#1}}}}
\newcommand{\ee}{\end{eqnarray}} 

% arrangement
\newcommand{\hide}[1]{}
\newcommand{\drawline}{\begin{picture}(500,1)\line(1,0){500}\end{picture}}
\newcommand{\bitem}{$\bullet$ \ \ \ }
\newcommand{\Cn}[1]{\begin{center} #1 \end{center}}
\newcommand{\mpg}[2][1.0\hsize]{\begin{minipage}[b]{#1}{#2}\end{minipage}}
\newcommand{\mpgt}[2][1.0\hsize]{\begin{minipage}[t]{#1}{#2}\end{minipage}}



% extra math commands by jarondl
\newcommand{\inner}[2]{\left \langle #1 \middle| #2\right\rangle} % Inner product

%fminipage using fancybox package
\newenvironment{fminipage}%
  {\begin{Sbox}\begin{minipage}}%
  {\end{minipage}\end{Sbox}\fbox{\TheSbox}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Page setup
\setlength{\parindent}{0cm} 
\setlength{\parskip}{0.3cm} 

%%% Sections. The original revtex goes like this:
%\def\section{%
%  \@startsection
%    {section}%
%    {1}%
%    {\z@}%
%    {0.8cm \@plus1ex \@minus .2ex}%
%    {0.5cm}%
%    {\normalfont\small\bfseries}%
%}%
%\def\subsection{%
%  \@startsection
%    {subsection}%
%    {2}%
%    {\z@}%
%    {.8cm \@plus1ex \@minus .2ex}%
%    {.5cm}%
%    {\normalfont\small\bfseries}%
%}%
%%%%%%% And our version goes like this:
\makeatletter
\def\section{%
  \@startsection
    {section}%
    {1}%
    {\z@}%
    {0.8cm \@plus1ex \@minus .2ex}%
    {0.5cm}%
    {\Large\bf $=\!=\!=\!=\!=\!=\;$}%
}%
\def\subsection{%
  \@startsection
    {subsection}%
    {2}%
    {\z@}%
    {.8cm \@plus1ex \@minus .2ex}%
    {.5cm}%
    {\normalfont\small\bfseries$=\!=\!=\!=\;$}%
}%
%%%%%%%%%%  Here we deal with capitalization. The original revtex first, and then our version.
%\def\@hangfrom@section#1#2#3{\@hangfrom{#1#2}\MakeTextUppercase{#3}}%
%\def\@hangfroms@section#1#2{#1\MakeTextUppercase{#2}}%
\def\@hangfrom@section#1#2#3{\@hangfrom{#1#2}{#3}}%
\def\@hangfroms@section#1#2{#1{#2}}%
\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Notes}

\author{Yaron de Leeuw, Doron Cohen}
\date{\today}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This work concerns the survival probality and spreading (defined in section \ref{sec:definitions}) for systems with multiple intercontected sites. We will study which physical and mathematical properties are necessary to describe the system. The systems are defined using the transition rates between sites (and sometimes the energy of the sites). Some sort of random distribution is used for building th systems, either by randomly setting the transition rates themselves, or by randomly positioning them in space, obtaining the transition rates from their distances. These physical systems are mapped to matrices, which are then further studied. The matrices have random origin, but are not completely random, due to physical or topological considirations.


In regular diffusion there is a known relation between the survival probability and the diffusion. 
A known solution for the diffusion constant in a nearest neighbor network is \cite{Derrida:1983}:
\begin{align}
D=\overline{w}_{harmonic} =(\overline{w^{-1}})^{-1}=\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}

%%%%%%%%%%%%
\section{Questions}
\begin{itemize}
    \item Is the known relation $\mathcal{P}(t) \sim \frac{1}{\sqrt{S(t)}}$ general, and does it hold for more than two dimensions?
    \item Do geometrical and topological considirations matter, or is the rate distribution enough to find the survival and spreading?
    \item Could a resistor network calculation be the generalization of Alexander's solution (see section {sec:alexander}) for more than $1d$ (Quasi $1d$ or even $d>1$?
    \item What are the implications of different site energies, and does the VRH method address them correctly?
\end{itemize}
%%%%%%%%%%%%
\section{Tentative working plan}
Several systems of interest are defined in section \ref{sec:sys_of_interest}. With those systems 
\begin{itemize}
    \item Solve $1d$ nearest neighbors problems, analytically, numerically, and using the resistor network method. The numerical solution is based on finding the eigenvalues.
    \item Solve quasi-1d and strip systems.
    \item Analyze $2d$ systems, analytically and numerically.
    \item VRH: Variable-Range-Hopping, insert different energies for the sites, 
    \item Resistor network calculation \ref{sec:res_net}. Could a resistor network calculation be the generalization of Alexander's solution for more than $1d$ (Quasi $1d$ or even $d>1$?

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%
\section{definitions}\label{sec:definitions}
\begin{description}
    \item[Site probability - $P_n(t)$ : ] The probability to be in site $n$ at time $t$.
    \item[Survival probability - $\mathcal{P}(t)$ : ] The probability to remain in the starting site. If the initial condition was $P_0(0)=1$, $P_i(0)=0 \textrm{  for  } i\neq 0$, then $\mathcal{P}(t)= P_0(t)$
    \item[Spreading - $S(t)$ : ] The squared width (second moment) of the probability distribution. $S(t) \equiv \sum_n n^2 P_n -\left(\sum_n n P_n\right)^2$. 
    \item[Diffusion coefficient - $D$ : ] The long term ratio of spreading to time, with $S(t) \sim 2Dt$.
    \item[Rate matrix - $W_{ij}$ : ] A matrix describing the transition rates between sites. $W_{ij}$ is the rate of transitions from site $j$ to site $i$.
    \item[Probability conserving matrix : ]\label{def:prob_conserv} A matrix where the sum of each and every row is zero - $\sum_i W_{ij} = 0 \qquad \forall j$. This relates to conserving probability if this matrix is a rate matrix.
    \item[Banded matrix, with width $b$ : ] A matrix that has non zero values only within the first $b$ off diagonals, meaning $2b+1$ diagonals with values.
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rate equation}

The basic model is of a particle distributed among many sites. The particle moves from site to site, with a transition rate matrix $W_{ij}$. Then, the probability to find the particle at a given site $i$, denoted $P_i$, obeys the rate equation:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t)
\end{align}
We can generally describe a system by defining its transition matrix $W_{ij}$.

%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systems of interest}\label{sec:sys_of_interest}

In the following sections we will describe several systems by defining their transition matrices. Unless stated otherwise, all the matrices are symmetric and probability conserving as defined previously \ref{sec:definitions}. The rates are obviously non negative.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{One dimensional nearest neighbor systems with wide distributions}

This class of systems has only nearest neighbor transitions, with non zero rates only on the first off diagonals. The rate distribution is wide, say $P(w) \sim \textrm{log wide}$.


If the transition rates are $w_1,w_2,w_3,...$, the matrix looks like this:
\begin{align}
W = 
\begin{pmatrix}
-w_1  & w_1 \\
w_1  & -w_1-w_2 &  w_2 \\
 & w_2 & -w_2-w_3 &  w_3 \\
& & w_3 & -w_3-w_4 & \; w_4 \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{One dimension, beyond nearest neighbors}

This system has non zero rates to sites beyond the nearest neighbors. The largest distance with non zero rates will define the matrix bandwidth $b$.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quasi one dimensional - strip}

This system is two dimensional, but one of the dimensions is very small compared to the other. This matrix will also be banded.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Two or higher dimensions}

This class of systems has the sites in a two (or more) dimensional system. An added difficulty is that there is more then one coordinate, so that there are several ways to map this to 1d vectors.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variable site energy}

Contrary to all the previous systems, here the energy per site is not constant. We can \cite{Ambegaokar:1971} accommodate for this, by including Boltzmann's factor in the transition rates. Another consequence is that the transition rates is not symmetric.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The relation between the survival probability and the eigenmode distribution}
The matrix has eigenmodes $V_\lambda$ with eigenvalues $\lambda$. 
We define a standard inner product :
\begin{align}
\inner{A}{B} = \sum_{ii} A_iB_i
\end{align}
Then we can write the probability vector $P$ as a sum of the normalized eigenmodes.
\begin{align}
P_n(t) = \sum_\lambda \inner{P(t)}{V_\lambda}{V_\lambda }_n = \sum_\lambda C_\lambda(t) {V_\lambda}_n
\end{align}
Where we have defined $C_\lambda(t)\equiv \inner{P(t)}{V_\lambda}$.

The rate equation becomes
\begin{align}
&\sum_\lambda \left( \frac{dC_\lambda(t)}{dt}{V_\lambda}_n - C_\lambda(t)\sum_m W_{nm}{V_\lambda}_n \right)= \sum_\lambda \left( \frac{dC_\lambda(t)}{dt} - \lambda C_\lambda(t)\right){V_\lambda}_n =0\\
&C_\lambda(t) = C_\lambda(0)e^{\lambda t}\\
&P_n(t) = \sum_\lambda C_\lambda(t) {V_\lambda}_n = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_n 
\end{align}
The survival probability is defined as the probability to remain in a specific site. If we begin with a probability vector whose only non zero element is $P_i(0)=1$, the corresponding $C_\lambda(0)$s will be:
\begin{align}
C_\lambda(0) = \inner{P(0)}{V_\lambda}= {V_\lambda}_ito
\end{align}
And then the survival probability will be:
\begin{align}
\mathcal{P}_i(t) = P_i(t) = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_i=\sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t} 
\end{align}
The mean survival probability will be:
\begin{align} \label{eq:surv_eigenvalues}
\mathcal{P}(t) = \sum_i \frac{1}{N} P_i(t)=\sum_i \frac{1}{N} \sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t}  =\frac{1}{N}\sum_\lambda e^{\lambda t} = \int g(\lambda) e^{\lambda t}
\end{align}
Where $g(\lambda)$ stands for the density of eigenvalues. This last results means that the survival probability is the Laplace transform of the eigenvalue distribution.


%%%%%%%%%%%%%%
\section{The relation between the diffusion coefficient and the survival probability}
If $D$ exists, the long time decay of $\mathcal{P}(t)$, according to Alexander et al \cite{Alexander:1981:RMP}, (and consistent with the scaling hypothesis $\mathcal{P}(t)\sqrt{S(t)} \propto 1$) is:
\begin{align}
    \mathcal{P}(t) = \frac{1}{\sqrt{4\pi D t}} \\
    g(\lambda) = \mathcal{L}^{-1}[\mathcal{P}(t)] = \mathcal{L}^{-1}\left[\frac{1}{\sqrt{4\pi D t}}\right]
\end{align}
Where $\mathcal{L}^{-1}$ is the inverse Laplace transform, (The Bromwich integral).
\begin{align}
    g(\lambda) = \mathcal{L}^{-1}\left[\frac{1}{\sqrt{4\pi Dt}}\right] = \frac{1}{\sqrt{4\pi^2 D\lambda}} \\
    \int_0^\lambda g(\lambda')d\lambda' = \sqrt{\frac{\lambda}{\pi^2 D}}
\end{align}

%%%%%%%%%%%%%%%%%55
\section{The harmonic mean of log normal distribution}
The harmonic mean of the log normal distribution with normal width $\sigma$ is given by:
\begin{align}
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}} \frac{1}{e^x} dx  = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}-x} dx \\
&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(x-\sigma^2\right)^2}{2\sigma^2}+\frac{\sigma^2}{2}} dx = e^{\frac{\sigma^2}{2}}\\
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = e^{-\frac{\sigma^2}{2}}
\end{align}
The generalization for $\mu\neq 0$ is :
\begin{align}
\overline{w}_{harmonic} = e^{-\frac{\sigma^2}{2}+\mu}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solving nearest neighbor alternating transition rates analytically}
In this section I solve the alternating transition rates problem exactly.

We can think of a model where every second transition is different, so the transitions are a,b,a,b,a,b... And the matrix is:
\begin{align}
W = 
\begin{pmatrix}
-a  & a \\
a  & -a-b &  b \\
 & b & -a-b &  a \\
& & a & -a-b & \; b \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
The diffusion coefficient according to \ref{eq:diff} should be:
\begin{align}\label{eq:D_alter}
D = \left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = \frac{2}{\frac{1}{a} + \frac{1}{b}} = \frac{2ab}{a+b}
\end{align}

Applying the eigenvalue equation to this matrix gives us two inidical equations, for even and odd $n$'s. If we take an odd $n$ the equations are:
\begin{align}
a {V_\lambda}_n - (a+b+\lambda){V_\lambda}_{n+1} + b {V_\lambda}_{n+2} =0  \\
b {V_\lambda}_{n+1} - (a+b+\lambda){V_\lambda}_{n+2} + a {V_\lambda}_{n+3} =0 
\end{align}
The solution with real eigenmodes is rather cumbersome, but the eigenvalues found by any set of orthogonal eigenmodes have to be the same. So for purely analytical reasons we find the eigenvalues with complex eigenmodes, although they have no meaning in our model.


The eigenvalues for this matrix can be found analytically, using Bloch's theorem. The solution for a periodic structure with a unit cell is ${V_\lambda}_n = e^{ikn} u(x)$ where $u(x)$ is a function with the periodicity of the unit cell. In our case this simplifies to:
\begin{align}
{V_\lambda}_n = \begin{cases}e^{ikn} c_1 \; &, \text{n odd} \\ e^{ikn} c_2 \; &, \text{n even} \end{cases}
\end{align}
Periodic boundary conditions dictate $k=\frac{2\pi}{N}m$ where $m$ is an integer.

When we apply the eigenvalue equation to this vector ($WV_\lambda = \lambda V_\lambda$), we obtain two linear equations:
\begin{align}
ac_1e^{-ik}   -(a+b)c_2 +bc_1e^{ik} &= \lambda c_2 \\
bc_2e^{-ik}   -(a+b)c_1 +ac_2e^{ik} &= \lambda c_1 
\end{align}
The solution for $\lambda$ is
\begin{align}
(a+b+\lambda)^2 = a^2+b^2+2ab\cos 2k \\
\lambda = -(a + b) \pm \sqrt{a^2+b^2+2ab\cos 2k}  \label{eq:abab_eigenvals}
\end{align}
We are interested in the eigenvalues close to zero, which means taking the plus sign, and $k\ll 1$:
\begin{align}
\lambda \approx -(a + b) + \sqrt{a^2+b^2+2ab(1-2k^2)} = -(a+b) +(a+b)\sqrt{1 - \frac{4abk^2}{(a+b)^2}}\approx \frac{2abk^2}{a+b}
\end{align}
The $k$'s are $k=\frac{2\pi}{N}m$, but with degeneracy of $2$ ($\pm k$ give the same result) and therefore the cumulative eigenvalues are:
\begin{align}
\sqrt{\lambda\frac{a+b}{2ab}}\frac{N}{\pi}
\end{align}
Or, using the diffusion coefficient we have found in \eqref{eq:D_alter}:
\begin{align}
\sqrt{\frac{\lambda}{D}}\frac{N}{\pi}
\end{align}

Now that we have the eigenvalues, we may use equation \ref{eq:surv_eigenvalues} to find the survival probability as a function of time:
\begin{align}
\mathcal{P}(t) =\int g(\lambda) e^{\lambda t} = \frac{1}{\sqrt{4 \pi D t}}
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The relation between the eigenvalue distribution and the transition matrix}
In the supplementary material of \cite{Amir:2010:PRL} the moments of the distribution of the eigenvalues are found. 

If we define $D$ as the diagonalized matrix $W$ as in $W=UDU^{-1}$, we have
\begin{align}\label{eq:eig_moments}
\sum_\lambda \lambda^k = Tr (D^k) = Tr(UDU^{-1}UDU^{-1}UDU^{-1}UDU^{-1}...) = Tr (W^k)
\end{align}
So the exponent $k$ of the matrix $W$ is equal to the $k^{th}$ moment of the distribution of the eigenvalues. If one can find the exponents of the matrix, he can find the moments of the distribution, and hopefully the distribution itself. 


%%%%%%%%%%%%%%
\section{Solving the spacing distribution for uniformly random points in d dimensions} \label{subsec:euclid}
Random Euclidean distance matrices were widely researched \cite{Mezard:1999:NPB}, [insert more refs..]. Amir et al \cite{Amir:2010:PRL} have studied a model with the exponents of euclidean distances. They obtain the survival probability by finding the distribution of the eigenvalues, using the idea of \ref{eq:eig_moments}. 

In one dimension, the transition rates are related to the distance between points distributed uniformly, with density $n$. The nearest neighbor distribution can be considered as the probability for no neighbor at $x'<x$, and a neighbor at $x<x'<x+dx$. Because the points are distributed uniformly, the probability for no neighbors at a small interval $\Delta x$ is :
\begin{align}P_0(\Delta x) = (1-n\Delta x) \end{align}
So for a larger interval, we can divide $x$ to $N$ parts:
\begin{align}P_0(x) = \lim_{N\rightarrow \infty}(1-\frac{nx}{N})^N = e^{-n x} \end{align}
The probability for a neighbor at $x<x'<x+dx$ is:
\begin{align}P_1(x<x'<x+dx) = n dx \end{align}
So the probability for a nearest neighbor between $x$ and $x+dx$ is:
\begin{align}P_{NN}(x< x'<x+dx) = P_0(x'<x)\cdot P_1(x<x'<x+dx) = e^{-n x}ndx \end{align}
And the distribution density functions is:
\begin{align} f(x) = e^{-n x}n\end{align}

For higher dimensions, if we assume that site density is constant (denoted $n$), then the probability for no neighbors in volume $V(r)$ is:
\begin{align} P_0( V(r)) = e^{-nV(r)}\end{align}
So the cumulative probability for the nearest neighbor distance to be less than $r$ is:
\begin{align} F (r) = 1-P_0(V(r)) = 1- e^{-nV(r)} \end{align}
The probability density is the derivative of the cumulative probability:
\begin{align} f(r) = \frac{dF(r)}{dr} =   e^{-nV(r)} n \frac{dV(r)}{dr} \end{align}
For two dimensions:
\begin{align} f(r) = e^{-n \pi r^2}n2\pi r \end{align}
For three dimensions:
\begin{align} f(r) = e^{-n \frac{4\pi}{3} r^3}n4\pi r^2 \end{align}
Amir et al. \cite{Amir:2010:PRL} come to the same equation in their supplementary material:

%%
\begin{fminipage}{\textwidth}
\begin{align}P_{nn}(r)=\frac{d
V_d}{r_{nn}} {(r/r_{nn})}^{d-1} e^{-V_d {(r/r_{nn})}^d}\end{align} 
where $V_d$ is the volume of a $d$ dimensional sphere and $r_{nn}$ is the
average nearest-neighbor distance.
\end{fminipage}

In our notation (replacing $V_d$ with the more conventional $\frac{\Omega_d}{d}$ ), we can write it as:
\begin{align} f(r) = e^{-n \frac{\Omega_d}{d} r^d} n\Omega_d r^{d-1} \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Finding the transition rate distribution for $w= e^{-r/ \xi}$, with random points}
We can change the random variable from $r$ to $w= e^{-r/ \xi}$, with $0<w<1$:
\begin{align}
    &|f_w(w)dw| = |f_r(r)dr| \\
    &f_w(w) = f_r(r)\left|\frac{dr}{dw}\right| = f_r(-\xi \ln w)\frac{\xi}{w} = e^{-n \frac{\Omega_d}{d} (-\xi\ln w)^d} n\Omega_d (-\xi\ln w)^{d-1}\frac{\xi}{w} \\
    &f_w(w) \sim \frac{|\ln w|^{d-1}}{w} e^{-|\ln w|^d}
\end{align}
In $1d$ that becomes:
\begin{align} f(w)= e^{n \xi\ln w} \frac{n\xi}{w} = w^{n\xi-1}n\xi\end{align}
Which matches Alexander's $\alpha = 1-n\xi$, and therefore should have survival of $\mathcal{P}(t)\propto t^{-\frac{1-\alpha}{2-\alpha}}= t^{-\frac{n\xi}{1+n\xi}}$. This result appears also in Amir's article.



For $2d$:
\begin{align} f(w) = e^{-n \pi (-\xi\ln w)^2} n 2\pi  (-\xi\ln w)\frac{\xi}{w}\end{align}

The diffusion coefficient is given by $\left(\overline{(w^{-1})}\right)^{-1}$:
\begin{align}
    & \overline{(w^{-1})} = \int_0^\infty \frac{1}{w} e^{-n \frac{\Omega_d}{d} r^d} n\Omega_d r^{d-1} dr = \int_0^\infty e^{-n \frac{\Omega_d}{d} r^d} n\Omega_d r^{d-1}e^{r/\xi} dr 
\end{align}
For $d>1$ this integral converges. If the naive $1d$ calculation holds, normal diffusion should occur.

Amir also finds the eigenvalue distribution (using the idea of equation \eqref{eq:eig_moments} ):

%%
\begin{fminipage}{\textwidth}
\begin{align} P(\lambda)=  \frac{\epsilon^d d V_d /2
(-\rm{log}(-\lambda/2))^{d-1} e^{-\frac{V_d}{2} \epsilon^d
(-\rm{log}(-\lambda/2))^d}}{\lambda} \end{align}


The cumulative $C(\lambda)$ of this distribution takes a
particularly simple form: 
\begin{align} C(\lambda)\equiv \int_\lambda^0
P(\lambda)d\lambda= e^{-\frac{V_d}{2} \epsilon^d
(-\rm{log}(-\lambda/2))^d} \end{align}
\end{fminipage}
Or in our notation:
\begin{align} C(\lambda)= e^{-\frac{\Omega_d}{d} n\xi^d \left( -\log(\frac{\lambda}{2})\right)^d}  \end{align}
In 2D that becomes:
\begin{align} C(\lambda) = e^{-\pi n\xi^2 \left(\log(\frac{\lambda}{2})\right)^2 } \end{align}
This cumulative distribution is the same as the log-normal probability distribution. 

%%%%%%%%%%%%%%%%%%%%%%
\section{The survival and spreading for the one dimensional problem}\label{sec:alexander}
We present the survival and spreading for several cases of 1d systems, based on the work by \textcite{Alexander:1981:RMP}.

Three classes are classified:
\begin{align} 
    &\mathrm{(a) : }\qquad (\overline{w^{-1}})^{-1} < \infty  \\
    &\mathrm{(b) : }\qquad P(w)\rightarrow \mathrm{const} \qquad\mathrm{as}\qquad w \rightarrow 0 \\
    &\mathrm{(c) : }\qquad P(w) = 
        \begin{cases} 
            (1-\alpha)w^{-\alpha} , & 0\le w \le 1 \\
            0, & \mathrm{otherwise}
        \end{cases}
        \qquad\mathrm{with}\qquad 0<\alpha<1
\end{align}
For two of the three classes the survival at $w\rightarrow 0$ and $t\rightarrow \infty$ are (6.13) [Where $D=(\overline{w^{-1}})^{-1}$ ]:
\begin{align}
&\mathrm{(a) : }\qquad \mathcal{P}(t) \approx \sqrt{\frac{1}{4\pi D t}}  \\
&\mathrm{(c) : }\qquad \mathcal{P}(t) \propto t^{-\frac{1-\alpha}{2-\alpha}}
\end{align}
The spreading is defined by (7.3):
\begin{align}\label{eq:spreading}
     S(t) = \overline{x^2(t)} = \sum_n n^2 P_n(t) 
\end{align}
And it is solved (7.18):
\begin{align}
    &\mathrm{(a) : }\qquad S(t) \approx 2Dt  \\
    &\mathrm{(b) : }\qquad S(t) \propto \frac{t}{\ln t}  \\
    &\mathrm{(c) : }\qquad S(t) \propto t^{2\frac{1-\alpha}{2-\alpha}}
\end{align}
Using a scaling hypothesis , he claims:
\begin{align}\label{eq:scaling_hypo}
S(t) \propto \left(\mathcal{P}(t)\right)^{-2} \qquad \mathrm{as}\qquad t\rightarrow \infty
\end{align}

%%%%%%%%%%
\section{Higher dimension considerations}
To solve the problem in higher dimensions, we label the sites with a 1d coordinate, and solve the matrix.

The claim in \eqref{eq:eig_moments} that $\sum_\lambda \lambda^k = Tr (W^k)$, is still be valid in higher dimensions. Therefore, the survival probability should stay the same.

The interesting question regards the spreading. If the hypothesis $S(t) \propto \left(\mathcal{P}(t)\right)^{-2}$ \eqref{eq:scaling_hypo} holds, the spreading should also stay the same. 

The definition of spreading Alexander uses \eqref{eq:spreading} ($\sum_n n^2 P_n(t)$) does match the average distance squared in 1D, but not in higher dimensions. The question is whether we can enumerate the sites in such a way that the methods presented so far apply.

%%%%%%%%%%%%%%%
\section{Defining sparsity?}
Stotland's \cite{Stotland:2010:PRB}  sparsity characterization is:\\*
%%%%%%%%%%%%%%%% minipage
\begin{fminipage}{\textwidth}
The sparsity of a matrix that consists of 
uncorrelated realizations can be characterized 
by a parameter~$s$ or optionally 
by the parameters~$p$ and~$q$ that 
are defined as follows: 
%
\begin{align}
s \ \ &=& \ \ \langle x \rangle^2 / \langle x^2 \rangle \\
p \ \ &=& \ \ \mbox{Prob}(x {>} \langle x\rangle) \\
q \ \ &=& \ \ \langle\langle x \rangle\rangle_{\tbox{median}}/\langle x \rangle
\end{align}
%
By this definition $p$ is the fraction of the elements that 
are larger than the algebraic average and $q$ is the ratio between the median
and the algebraic average.
We regard a matrix as sparse if $s\ll1$  
or equivalently if $p \ll 1$ or $q \ll 1$. 

\end{fminipage}
%%
\\

The direct calculation for the survival in a log normal banded matrix is depicted in figure \ref{fig:p_lognormal_band}. For each value of $b$, a matrix (currently $100
\times 100$) with a band b filled with log-normally distributed ($\sigma=1$) values was constructed. Then, the eigenvalues were numerically calculated, and then the Laplace sum $\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t}$.
%%%
%\begin{figure}[h!]
%\includegraphics[clip, width=0.9\hsize]{P_lognormal_band.pdf}
%\caption{$\mathcal{P}(t)$ for constant sparsity, and variable bandwidth. The survival was calculated using $\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t}$ \eqref{eq:surv_eigenvalues} with numerical diagonalization of the transition matrix.
%}
%\label{fig:p_lognormal_band}
%\end{figure}[h!]

%%%%%%%%%%
\section{Resistor Network Calculation} \label{sec:res_net}
Our basic equation is 
\begin{align} \boldsymbol{ \dot P } = \boldsymbol{W} \boldsymbol{P} \end{align}
It can be rewritten as:
\begin{align}  \boldsymbol{P} = \boldsymbol{W}^{-1}\boldsymbol{ \dot P } \end{align}
Which resembles resistivity calculations:
\begin{align} \boldsymbol{V} = \boldsymbol{R}\boldsymbol{ I } \end{align}
If we define the inverse resistivity as:
\begin{align} \boldsymbol{R} = \boldsymbol{W}^{-1} \end{align}
So our calculations become similar to those of resistors, whose rules are well known.

The resistor network calculation has been used by Stotland to find the diffusion of quasi one dimensional systems.

To solve the problem, we need to find the inverse of $\boldsymbol{W}^{-1}$. Such an inverse does not exist, because $\boldsymbol{W}$ always has a zero valued eigenmode.

%%%%%%%%
\section{Random Matrix Construction}\label{sec:matrix_construction}
To create a $N\times N$ matrix, we have to choose $N-1$ random numbers. To make sure our distribution is truly log-normal, we create $N-1$ numbers forming a log-normal distribution, and permute them. In order to create the numbers, first we create a vector of $N-1$ linearly spaced values between $0$ and $1$. Then we apply element-wise the inverse cumulative distribution function of the normal distribution \ref{eq:invcdf}, using the inverse complementary error function. This gives a $N-1$ vector with normal distributed values. The vector is then scaled ($\tilde{y} = \mu+\sigma\cdot y$) to the appropriate width and mean [of the normal distribution]. The last step is to permute the vector, and apply an exponent element-wise.
\begin{align}\label{eq:invcdf}
y = -\sqrt{2}\cdot\operatorname{erfcinv}(2\cdot x) 
\end{align}

To check ourselves, we have plotted the eigenvalue distribution for the lognoraml banded matrix, and a uniform random matrix in figure \ref{fig:eigenvalue_distribution}, and also compared between $b=5$ and $b=10$ in figure \ref{fig:eigenvalue_distribution_5_10}
\begin{figure}[h!]
    \subfloat[Uniform distribution ]{\includegraphics[clip, width=0.9\hsize]{eigvals_uniform}}
    \\ \subfloat[Lognoraml nearest neighbor matrix]{\includegraphics[clip, width=0.9\hsize]{eigvals_lognormal_loglog}}
    \caption{Cumulative eigenvalue distribution.}
    \label{fig:eigenvalue_distribution}
\end{figure}[h!]
\begin{figure}[h!]
    \includegraphics[clip,width=0.9\hsize]{eigvals_lognormal_normal}
    \caption{Cumulative eigenvalue distribution, in a normal scale}
\end{figure}[h!]
\begin{figure}[h!]
    \includegraphics[clip, width=0.9\hsize]{eigvals_ones_loglog}
\end{figure}[h!]
\begin{figure}[h!]
    \includegraphics[clip, width=0.9\hsize]{eigvals_alter_loglog}
\end{figure}[h!]
\begin{figure}[h!]
    \includegraphics[clip, width=0.9\hsize]{eigvals_box_loglog}
\end{figure}[h!]

%%%%%%%%%%%%%
\section{Geometry's role} \label{sec:geometry}
Random points were chosen on a torus. A matrix with transition rates $W_{ij} = e^{-r_{ij}}$ was constructed, for which eigenvalues and survival were calculated. Later, the matrix elements were permuted, and the same calculations were made. \ref{fig:distance}. Since the matrix should remain symmetric, the elements distribution should stay the same, and the sum of each row should stay $0$, the permutation was done on the upper triangle of the matrix only. (And then transposed to the other triangle, and the diagonal set match the zero sum demand).

%%%%%%%%%%%%
\section{Resistor network computation for 2d surfaces}
Here I will explain the dependence of $G(r)$ in $r$, so we could be able to use the resistor network calculation for $2d$ problems.

The basic idea, as in $1d$ networks, is to add a source of current ($+1$) and a sink ($-1$). If we can calculate the voltage between those "electrodes", we know the conductance, as $G = \frac{I}{V}$. 

Instead of calculating the system with both current connections, we can think of each of them seperately, and then superpose them. If a current goes from a single spot to infinity, the current should have radial symmetry. If so, the voltage at distance $r$ will be :
\begin{align}
  E = \frac{J}{\sigma} = \frac{I}{2\pi r\sigma} \\
  V = \int E dr = \frac{I}{2\pi\sigma}\ln\frac{r}{r_0}
\end{align}
The superposition of the sink just doubles the result, and we obtain:
\begin{align}
    G = \frac{I}{V} = \frac{\pi\sigma }{\ln\frac{r}{r_0}}  \\
    \sigma = \frac{1}{\pi}G\ln\frac{r}{r_0}
\end{align}


%%%%%%%%%%%
\section{After the Research Proposal - Spacing statistics}
We are working on a model with the following rates:
\begin{align}
    w(r) = w_0e^{(1-\frac{r}{r_0})/\epsilon}\\
    \frac{r}{r_0} = 1 - \epsilon\ln \left(\frac{w}{w_0}\right)
\end{align}
The spacing (nearest neighbor) statistics have the following cummulative distribution:
\begin{align}
    C(r) = 1-e^{-\frac{\Omega_d}{d}\left(\frac{r}{r_0}\right)^2}
\end{align}
So the cummulative distribution of the nearest neighbor rates is:
\begin{align}
    C(r(w)) &= \exp\left[{-\frac{\Omega_d}{d}\left(1-\epsilon\ln \left(\frac{w}{w_0}\right)\right)^d}\right] \\
&= \begin{cases} 
    e^{-2}w^{2\epsilon} &d=1 \\
e^{-\pi(1-\epsilon\ln w)^2} &d=2 \\
e^{-\frac{4}{3}\pi(1-\epsilon\ln w)^3} &d=3 \\
   \end{cases}
\end{align}
With $w < \exp(\frac{1}{\epsilon})$.

%\bibliographystyle{plainnat}
\bibliography{jarondl}
\end{document}
