%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[onecolumn,fleqn]{revtex4}


% special 
\usepackage{ifthen}
\usepackage{ifpdf}
\usepackage{float}
\usepackage{color}

% fonts
\usepackage{latexsym}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{bm}
\usepackage{wasysym}


\ifpdf
\usepackage{graphicx}
\usepackage{epstopdf}
\else
\usepackage{graphicx}
\usepackage{epsfig}
\fi

%by jarondl
\usepackage{verbatim} % for multiline comments
\usepackage{natbib}
\usepackage{fancybox}
\usepackage{cmap}  % for making pdf mathmode searchable
\usepackage[pdftitle={Notes by Jarondl}]{hyperref}  % for hyperlinks in biblio. should be called last?

\graphicspath{{figures/},{PROG/figures/}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% NEW 
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\varphiJ}{\bm{\varphi}}
\newcommand{\thetaJ}{\bm{\theta}}
%\renewcommand{\includegraphics}[2][0]{FIGURE}


% math symbols I
\newcommand{\sinc}{\mbox{sinc}}
\newcommand{\const}{\mbox{const}}
\newcommand{\trc}{\mbox{trace}}
\newcommand{\intt}{\int\!\!\!\!\int }
\newcommand{\ointt}{\int\!\!\!\!\int\!\!\!\!\!\circ\ }
\newcommand{\ar}{\mathsf r}
\newcommand{\im}{\mbox{Im}}
\newcommand{\re}{\mbox{Re}}

% math symbols II
\newcommand{\eexp}{\mbox{e}^}
\newcommand{\bra}{\left\langle}
\newcommand{\ket}{\right\rangle}

% Mass symbol
\newcommand{\mass}{\mathsf{m}} 
\newcommand{\rdisc}{\epsilon} 

% more math commands
\newcommand{\tbox}[1]{\mbox{\tiny #1}}
\newcommand{\bmsf}[1]{\bm{\mathsf{#1}}} 
\newcommand{\amatrix}[1]{\begin{matrix} #1 \end{matrix}} 
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% equations
\newcommand{\be}[1]{\begin{eqnarray}\ifthenelse{#1=-1}{\nonumber}{\ifthenelse{#1=0}{}{\label{e#1}}}}
\newcommand{\ee}{\end{eqnarray}} 
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}} 



% arrangement
\newcommand{\drawline}{\begin{picture}(500,1)\line(1,0){500}\end{picture}}
\newcommand{\bitem}{$\bullet$ \ \ \ }
\newcommand{\Cn}[1]{\begin{center} #1 \end{center}}
\newcommand{\mpg}[2][1.0\hsize]{\begin{minipage}[b]{#1}{#2}\end{minipage}}
\newcommand{\mpgt}[2][1.0\hsize]{\begin{minipage}[t]{#1}{#2}\end{minipage}}

\newcommand{\rmrk}[1]{\textcolor{red}{#1}}
\newcommand{\Rmrk}[1]{\textcolor{blue}{\LARGE\bf #1}}
\newcommand{\hide}[1]{\rmrk{[hidden text]}}

% extra math commands by jarondl
%\newcommand{\aket}[1]{\left| #1 \right\rangle}
%\newcommand{\abra}[1]{\left\langle #1 \right|}
%\newcommand{\abraket}[2]{\left\langle #1 | #2   \right\rangle}
%\newcommand{\sandwich}[3]{\left\langle #1 | #2 | #3  \right\rangle}
%\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\inner}[2]{\left \langle #1 \middle| #2\right\rangle} % Inner product

%fminipage using fancybox package
\newenvironment{fminipage}%
  {\begin{Sbox}\begin{minipage}}%
  {\end{minipage}\end{Sbox}\fbox{\TheSbox}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Notes}

\author{Yaron de Leeuw, Doron Cohen}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The mathematical properties of random matrices may offer interesting insight for the physical properties. Such physical properties include the diffusion and the Survival Probability, while the mathematical properties include the sparsity of the matrix and its spectrum. 

We shall concentrate on sparse matrices with log-normally distributed values. At first, we will work on a Nearest-Neighbor model as described in section \ref{sec:our_model}. One can think of this model as a one dimensional chain of sites, where transition rates between the sites vary log-normally.

In regular diffusion there is a known relation between the survival probability and the diffusion. 
A known solution for the diffusion constant in a nearest neighbor network is \cite{Derrida:1983}:
\begin{align}
D=\overline{w}_{harmonic} =(\overline{w^{-1}})^{-1}=\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}
A major problem with this solution is that not all distributions have a defined harmonic average. The question is whether those other distributions show regular diffusion, and if so, does the relation between survival probability and diffusion hold?


Another interesting issue is that of dimensions higher than 1. Inherently, higher dimensions offer different transition rate distributions. What is not clear is whether the dimensionality of the system adds other complexities or not. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Master equation}
The equation governing the time development in a transition rate problem is:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t)
\end{align}
Where $P_n(t) \ge 0$ is the probability to be at site $n$ at time $t$ and $W$ is the transition matrix.

The matrix has eigenmodes $V_\lambda$ with eigenvalues $\lambda$. 
We define a standard inner product :
\begin{align}
\inner{A}{B} = \sum_{ii} A_iB_i
\end{align}
Then we can write the probability vector $P$ as a sum of the normalized eigenmodes.
\begin{align}
P_n(t) = \sum_\lambda \inner{P(t)}{V_\lambda}{V_\lambda }_n = \sum_\lambda C_\lambda(t) {V_\lambda}_n
\end{align}
Where we have defined $C_\lambda(t)\equiv \inner{P(t)}{V_\lambda}$.


The master equation becomes
\begin{align}
&\sum_\lambda \left( \frac{dC_\lambda(t)}{dt}{V_\lambda}_n - C_\lambda(t)\sum_m W_{nm}{V_\lambda}_n \right)= \sum_\lambda \left( \frac{dC_\lambda(t)}{dt} - \lambda C_\lambda(t)\right){V_\lambda}_n =0\\
&C_\lambda(t) = C_\lambda(0)e^{\lambda t}\\
&P_n(t) = \sum_\lambda C_\lambda(t) {V_\lambda}_n = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_n 
\end{align}
%%%%%%%%%%%%%
\subsection{Survival probability}
The survival probability is defined as the probability to remain in a specific site. If we begin with a probability vector whose only non zero element is $P_i(0)=1$, the corresponding $C_\lambda(0)$s will be:
\begin{align}
C_\lambda(0) = \inner{P(0)}{V_\lambda}= {V_\lambda}_ito
\end{align}
And then the survival probability will be:
\begin{align}
\mathcal{P}_i(t) = P_i(t) = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_i=\sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t} 
\end{align}
The mean survival probability will be:
\begin{align} \label{eq:surv_eigenvalues}
\mathcal{P}(t) = \sum_i \frac{1}{N} P_i(t)=\sum_i \frac{1}{N} \sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t}  =\frac{1}{N}\sum_\lambda e^{\lambda t}
\end{align}
%%%%%%%%%%%%%%%
\subsection{Spreading}
The spreading of the probabilities is defined as the second moment of the locations:
\begin{align}
S(t) = \sum_n n^2 P_n -\left(\sum_n n P_n\right)^2
\end{align}
Alexander \cite{Alexander:1981:RMP} has shown that if the average of inverse transition rates is bounded ($\left(\frac{1}{N}\sum\frac{1}{w_n}\right) < \infty$) then the a spreading at $t\rightarrow \infty$ behaves like in an ordered chain:
\begin{align}
S(t) \approx 2Dt 
\end{align}
With a diffusion coefficient 
\begin{align}\label{eq:diff}
D = \left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}

%%%%%%%%%%%%%%
\subsection{The relation between the spreading and the survival probability}
We have two known quantities, one is the survival probability $\mathcal{P}(t)$, which is obtained by the use of eigenmode decomposition, and the other is the spreading $S(t)$, which was derived from the harmonic mean. The relation between them is not always clear. 

The scaling hypothesis ??? shows that
\begin{align}
\mathcal{P}(t)\sqrt{S(t)}=1
\end{align}
On the other hand, we may be able to derive the spreading using the same methods we have used for the survival probability, i.e. eigenmode decomposition.

By definition:
\begin{align}
S(t) = \sum_n n^2 P_n -\left(\sum_n n P_n\right)^2 = \sum_n\sum_\lambda C_\lambda(0)e^{\lambda t}n^2 {V_\lambda}_n - (\sum_n\sum_\lambda C_\lambda(0)e^{\lambda t}n {V_\lambda}_n)^2
\end{align}
If we at start a specific site $i$, so that the only non zero probability is $P_i(0)=1$, the spreading will become:
\begin{align}
S_i(t) = \sum_n\sum_\lambda e^{\lambda t}n^2 {V_\lambda}_n{V_\lambda}_i - (\sum_n\sum_\lambda e^{\lambda t}n {V_\lambda}_n{V_\lambda}_i)^2
\end{align}
Averaging over the initial state $i$ we have:
\begin{align}
\overline{S}(t) = \frac{1}{N}\sum_i\sum_n\sum_\lambda e^{\lambda t}n^2 {V_\lambda}_n{V_\lambda}_i - \frac{1}{N}\sum_i(\sum_n\sum_\lambda e^{\lambda t}n {V_\lambda}_n{V_\lambda}_i)^2
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Eigenvalue distribution}
In the supplementary material of \cite{Amir:2010:PRL} the moments of the distribution of the eigenvalues are found. 

If we define $D$ as the diagonalized matrix $W$ as in $W=UDU^{-1}$, we have
\begin{align}\label{eq:eig_moments}
\sum_\lambda \lambda^k = Tr (D^k) = Tr(UDU^{-1}UDU^{-1}UDU^{-1}UDU^{-1}...) = Tr (W^k)
\end{align}
So the exponent $k$ of the matrix $W$ is equal to the $k^{th}$ moment of the distribution of the eigenvalues. If one can find the exponents of the matrix, he can find the moments of the distribution, and hopefully the distribution itself. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models}\label{sec:our_model}
The model consists of a chain of connected sites. Our point of interest are the transition rates between those sites. One can define a master equation:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t) \\
W_{n,n-1} = W_{n-1,n} = w_n \\
W_{nn} = -w_n - w_{n+1}
\end{align}
Where the $w_n$s are the individual transition rates.

Less abstractly, the transition matrix is:
\begin{align}
W = 
\begin{pmatrix}
-w_1  & w_1 \\
w_1  & -w_1-w_2 &  w_2 \\
 & w_2 & -w_2-w_3 &  w_3 \\
& & w_3 & -w_3-w_4 & \; w_4 \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
%%%%%%%
\subsection{Log-normal}
The different $w_n$s are chosen from a log-normal distribution (by a special construction, see section \ref{sec:matrix_construction})

The harmonic mean of the log normal distribution with normal width $\sigma$ is given by:
\begin{align}
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}} \frac{1}{e^x} dx  = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}-x} dx \\
&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(x-\sigma^2\right)^2}{2\sigma^2}+\frac{\sigma^2}{2}} dx = e^{\frac{\sigma^2}{2}}\\
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = e^{-\frac{\sigma^2}{2}}
\end{align}
The generalization for $\mu\neq 0$ is :
\begin{align}
\overline{w}_{harmonic} = e^{-\frac{\sigma^2}{2}+\mu}
\end{align}
%%%%%
\subsection{Alternating transition rates}
We can think of a model where every second transition is different, so the transitions are a,b,a,b,a,b... And the matrix is:
\begin{align}
W = 
\begin{pmatrix}
-a  & a \\
a  & -a-b &  b \\
 & b & -a-b &  a \\
& & a & -a-b & \; b \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
The diffusion coefficient according to \ref{eq:diff} should be:
\begin{align}
D = \left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = \frac{2}{\frac{1}{a} + \frac{1}{b}} = \frac{2ab}{a+b}
\end{align}

Applying the eigenvalue equation to this matrix gives us two inidical equations, for even and odd $n$'s. If we take an odd $n$ the equations are:
\begin{align}
a {V_\lambda}_n - (a+b+\lambda){V_\lambda}_{n+1} + b {V_\lambda}_{n+2} =0  \\
b {V_\lambda}_{n+1} - (a+b+\lambda){V_\lambda}_{n+2} + a {V_\lambda}_{n+3} =0 
\end{align}
The solution with real eigenmodes is rather cumbersome, but the eigenvalues found by any set of orthogonal eigenmodes have to be the same. So for purely analytical reasons we find the eigenvalues with complex eigenmodes, although they have no meaning in our model.


The eigenvalues for this matrix can be found analytically, using bloch's theorem. The solution for a periodic structure with a unit cell is ${V_\lambda}_n = e^{ikn} u(x)$ where $u(x)$ is a function with the periodicity of the unit cell. In our case this simplifies to:
\begin{align}
{V_\lambda}_n = \begin{cases}e^{ikn} c_1 \; &, \text{n odd} \\ e^{ikn} c_2 \; &, \text{n even} \end{cases}
\end{align}
When we apply the eigenvalue equation to this vector ($WV_\lambda = \lambda V_\lambda$), we obtain two linear equations:
\begin{align}
ac_1e^{-ik}   -(a+b)c_2 +bc_1e^{ik} &= \lambda c_2 \\
bc_2e^{-ik}   -(a+b)c_1 +ac_2e^{ik} &= \lambda c_1 
\end{align}
The solution for $\lambda$ is
\begin{align}
(a+b+\lambda)^2 = a^2+b^2+2ab\cos 2k \\
\lambda = -(a + b) \pm \sqrt{a^2+b^2+2ab\cos 2k}  \label{eq:abab_eigenvals}
\end{align}
Now that we have the eigenvalues, we may use equation \ref{eq:surv_eigenvalues} to find the survival probability as a function of time:
\begin{align}
\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t} 
\end{align}

%%%%%%%%%%%
\subsection{Log normal, but with less nodes}
Perhaps a model similar to the original model but with a small finite number of nodes is easier to solve analytically.


%%%%%%%%%%%%%%
\subsection{Euclidean distance rates, or functions of it}
Random Euclidean distance matrices were widely researched \cite{Mezard:1999:NPB}, [insert more refs..]. Amir et al \cite{Amir:2010:PRL} have studied a model with the exponents of euclidean distances. They obtain the survival probability by finding the distribution of the eigenvalues, using the idea of \ref{eq:eig_moments}. 

In one dimension, the transition rates are related to the distance between points distributed uniformly, with density $n$. The nearest neighbor distribution can be considered as the probability for no neighbor at $x'<x$, and a neighbor at $x<x'<x+dx$. Because the points are distributed uniformly, the probability for no neighbors at a small interval $\Delta x$ is :
\[P_0(\Delta x) = (1-n\Delta x) \]
So for a larger interval, we can divide $x$ to $N$ parts:
\[P_0(x) = \lim_{N\rightarrow \infty}(1-\frac{nx}{N})^N = e^{-n x} \]
The probability for a neighbor at $x<x'<x+dx$ is:
\[P_1(x<x'<x+dx) = n dx \]
So the probability for a nearest neighbor between $x$ and $x+dx$ is:
\[P_{NN}(x< x'<x+dx) = P_0(x'<x)\cdot P_1(x<x'<x+dx) = e^{-n x}ndx \]
And the distribution density functions is:
\[ f(x) = e^{-n x}n\]

For higher dimensions, if we assume that site density is constant (denoted $n$), then the probability for no neighbors in volume $V(r)$ is:
\[ P_0( V(r)) = e^{-nV(r)}\]
So the cumulative probability for the nearest neighbor distance to be less than $r$ is:
\[ F (r) = 1-P_0(V(r)) = 1- e^{-nV(r)} \]
The probability density is the derivative of the cumulative probability:
\[ f(r) = \frac{dF(r)}{dr} =   e^{-nV(r)} n \frac{dV(r)}{dr} \]
For two dimensions:
\[ f(r) = e^{-n \pi r^2}n2\pi r \]
For three dimensions:
\[ f(r) = e^{-n \frac{4\pi}{3} r^3}n4\pi r^2 \]
Amir et al. \cite{Amir:2010:PRL} come to the same equation in their supplementary material:

%%
\begin{fminipage}{\textwidth}
\[P_{nn}(r)=\frac{d
V_d}{r_{nn}} {(r/r_{nn})}^{d-1} e^{-V_d {(r/r_{nn})}^d}\] 
"where $V_d$ is the volume of a $d$ dimensional sphere and $r_{nn}$ is the
average nearest-neighbor distance." 
\end{fminipage}

In our notation (still keeping $V_d$ for a $d$ dimensional unit sphere ), we can write it as:
\[ f(r) = e^{-n V_d r^d} ndV_d r^{d-1} \]

%%%

We can change the random variable from $r$ to $w= e^{-r/ \xi}$:
\begin{align*}
    &|f_w(w)dw| = |f_r(r)dr| \\
    &f_w(w) = f_r(r)\left|\frac{dr}{dw}\right| = f_r(-\xi \ln w)\frac{\xi}{w} = e^{-nV_d (-\xi\ln w)^d} ndV_d (-\xi\ln w)^{d-1}\frac{\xi}{w}
\end{align*}
In $1d$ that becomes:
\[ f(w)= e^{n \xi\ln w} \frac{n\xi}{w} = w^{n\xi-1}n\xi\]
Which matches Alexander's $\alpha = 1-n\xi$, and therefore should have survival of $\mathcal{P}(t)\propto t^{-\frac{1-\alpha}{2-\alpha}}= t^{-\frac{n\xi}{1+n\xi}}$. This result appears also in Amir's article.

For $2d$:
\[ f(w) = e^{-n \pi (-\xi\ln w)^2} n 2\pi  (-\xi\ln w)\frac{\xi}{w}\]

%%%%%%%%%%%%%%%%%%%%%%
\section{S. Alexander's treatment}
This is copied from Alexander's paper for easier referencing. 


S. Alexander provided a general solution for the problem \cite{Alexander:1981:RMP}. In his equation (2.4) three classes are classified:
\begin{align} 
    &\mathrm{(a) : }\qquad (\overline{w^{-1}})^{-1} < \infty  \\
    &\mathrm{(b) : }\qquad P(w)\rightarrow \mathrm{const} \qquad\mathrm{as}\qquad w \rightarrow 0 \\
    &\mathrm{(c) : }\qquad P(w) = 
        \begin{cases} 
            (1-\alpha)w^{-\alpha} , & 0\le w \le 1 \\
            0, & \mathrm{otherwise}
        \end{cases}
        \qquad\mathrm{with}\qquad 0<\alpha<1
\end{align}
For two of the three classes the survival at $w\rightarrow 0$ and $t\rightarrow \infty$ are (6.13):
\begin{align}
&\mathrm{(a) : }\qquad \mathcal{P}(t) \approx \sqrt{\frac{(\overline{w^{-1}})}{4\pi}}\frac{1}{\sqrt{t}}  \\
&\mathrm{(c) : }\qquad \mathcal{P}(t) \propto t^{-\frac{1-\alpha}{2-\alpha}}
\end{align}
The spreading is defined by (7.3):
\begin{align}\label{eq:spreading}
     S(t) = \overline{x^2(t)} = \sum_n n^2 P_n(t) 
\end{align}
And it is solved (7.18):
\begin{align}
    &\mathrm{(a) : }\qquad S(t) \approx 2(\overline{w^{-1}})^{-1}t  \\
    &\mathrm{(b) : }\qquad S(t) \propto \frac{t}{\ln t}  \\
    &\mathrm{(c) : }\qquad S(t) \propto t^{2\frac{1-\alpha}{2-\alpha}}
\end{align}
Using a scaling hypothesis , he claims:
\begin{align}\label{eq:scaling_hypo}
S(t) \propto \left(\mathcal{P}(t)\right)^{-2} \qquad \mathrm{as}\qquad t\rightarrow \infty
\end{align}

%%%%%%%%%%
\section{Higher dimension considerations}
To solve the problem in higher dimensions, we label the sites with a 1d coordinate, and solve the matrix.

The claim in \eqref{eq:eig_moments} that $\sum_\lambda \lambda^k = Tr (W^k)$, is still be valid in higher dimensions. Therefore, the survival probability should stay the same.

The interesting question regards the spreading. If the hypothesis $S(t) \propto \left(\mathcal{P}(t)\right)^{-2}$ \eqref{eq:scaling_hypo} holds, the spreading should also stay the same. 

The definition of spreading Alexander uses \eqref{eq:spreading} ($\sum_n n^2 P_n(t)$) does match the average distance squared in 1D, but not in higher dimensions. The question is whether we can enumerate the sites in such a way that the methods presented so far apply.

%%%%%%%%
\section{Quasi 1D}\label{sec:quasi1d}
A quasi $1d$ network is similar to a $1d$ network, but transitions beyond nearest neighbor are possible. The transition rate matrix $W$ will be a sparse, banded matrix. 

We define the band width $b$ as the farthest off-diagonal containing non zero elements, or as the largest distance between neighbors with non zero transition rates. The sparisity of a matrix has no accepted definition, but there are some measures available.


Stotland's \cite{Stotland:2010:PRB}  sparsity characterization is:\\*
%%%%%%%%%%%%%%%% minipage
\begin{fminipage}{\textwidth}
The sparsity of a matrix that consists of 
uncorrelated realizations can be characterized 
by a parameter~$s$ or optionally 
by the parameters~$p$ and~$q$ that 
are defined as follows: 
%
\begin{align*}
s \ \ &=& \ \ \langle x \rangle^2 / \langle x^2 \rangle \\
p \ \ &=& \ \ \mbox{Prob}(x {>} \langle x\rangle) \\
q \ \ &=& \ \ \langle\langle x \rangle\rangle_{\tbox{median}}/\langle x \rangle
\end{align*}
%
By this definition $p$ is the fraction of the elements that 
are larger than the algebraic average and $q$ is the ratio between the median
and the algebraic average.
We regard a matrix as sparse if $s\ll1$  
or equivalently if $p \ll 1$ or $q \ll 1$. 

\end{fminipage}
%%
\\

The direct calculation for the survival in a log normal banded matrix is depicted in figure \ref{fig:p_s_lognormal_band}. For each value of $b$, a matrix (curently $100
\times 100$) with a band b filled with lognormally distributed ($\sigma=1$) values was constructed. Then, the eigenvalues were numerically calculated, and then the laplace sum $\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t}$.
%%%
\begin{figure}
\includegraphics[clip, width=0.9\hsize]{P_lognormal_band.pdf}
\caption{$\mathcal{P}(t)$ for constant sparisity, and variable bandwidth. The survival was calculated using $\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t}$ \eqref{eq:surv_eigenvalues} with numerical diagonalization of the transition matrix.
}
\label{fig:p_lognormal_band}
\end{figure}


%%%%%%%%
\section{Random Matrix Construction}\label{sec:matrix_construction}
To create a $N\times N$ matrix, we have to choose $N-1$ random numbers. To make sure our distribution is truly log-normal, we create $N-1$ numbers forming a log-normal distribution, and permute them. In order to create the numbers, first we create a vector of $N-1$ linearly spaced values between $0$ and $1$. Then we apply element-wise the inverse cumulative distribution function of the normal distribution \ref{eq:invcdf}, using the inverse complementary error function. This gives a $N-1$ vector with normal distributed values. The vector is then scaled ($\tilde{y} = \mu+\sigma\cdot y$) to the appropriate width and mean [of the normal distribution]. The last step is to permute the vector, and apply an exponent element-wise.
\begin{align}\label{eq:invcdf}
y = -\sqrt{2}\cdot\operatorname{erfcinv}(2\cdot x) 
\end{align}

To check ourselves, we have plotted the eigenvalue distribution for the lognoraml banded matrix, and a uniform random matrix in figure \ref{fig:eigenvalue_distribution}
\begin{figure}
    \includegraphics[clip, width=0.9\hsize]{eigvals}
    \caption{Cummulative eigenvalue distribution for lognoraml banded matrix ($\sigma=1$, $b=4$) , and uniform distribution $[-1,1]$}
    \label{fig:eigenvalue_distribution}
\end{figure}

\section{Prelimanary, no statement yet}
We have also plotted the eigenvalue and survival for points on a torus in figure \ref{fig:distance}.
\begin{figure}
    \includegraphics[clip, width=0.9\hsize]{distance}
    \caption{Survival probability and cummulative eigenvalue distribution for points on a torus with $W_{ij} = e^{-r_{ij}}$}
    \label{fig:distance}
\end{figure}

\bibliographystyle{plainnat}
\bibliography{jarondl}
\end{document}
