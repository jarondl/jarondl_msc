%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[onecolumn,fleqn]{revtex4}

%by jarondl
\usepackage{verbatim} % for multiline comments
% special 
\usepackage{ifthen}
\usepackage{ifpdf}
\usepackage{float}
\usepackage{color}

% fonts
\usepackage{latexsym}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{bm}
\usepackage{wasysym}


\ifpdf
\usepackage{graphicx}
\usepackage{epstopdf}
\else
\usepackage{graphicx}
\usepackage{epsfig}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% NEW 
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\varphiJ}{\bm{\varphi}}
\newcommand{\thetaJ}{\bm{\theta}}
%\renewcommand{\includegraphics}[2][0]{FIGURE}


% math symbols I
\newcommand{\sinc}{\mbox{sinc}}
\newcommand{\const}{\mbox{const}}
\newcommand{\trc}{\mbox{trace}}
\newcommand{\intt}{\int\!\!\!\!\int }
\newcommand{\ointt}{\int\!\!\!\!\int\!\!\!\!\!\circ\ }
\newcommand{\ar}{\mathsf r}
\newcommand{\im}{\mbox{Im}}
\newcommand{\re}{\mbox{Re}}

% math symbols II
\newcommand{\eexp}{\mbox{e}^}
\newcommand{\bra}{\left\langle}
\newcommand{\ket}{\right\rangle}

% Mass symbol
\newcommand{\mass}{\mathsf{m}} 
\newcommand{\rdisc}{\epsilon} 

% more math commands
\newcommand{\tbox}[1]{\mbox{\tiny #1}}
\newcommand{\bmsf}[1]{\bm{\mathsf{#1}}} 
\newcommand{\amatrix}[1]{\begin{matrix} #1 \end{matrix}} 
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% equations
\newcommand{\be}[1]{\begin{eqnarray}\ifthenelse{#1=-1}{\nonumber}{\ifthenelse{#1=0}{}{\label{e#1}}}}
\newcommand{\ee}{\end{eqnarray}} 
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}} 



% arrangement
\newcommand{\drawline}{\begin{picture}(500,1)\line(1,0){500}\end{picture}}
\newcommand{\bitem}{$\bullet$ \ \ \ }
\newcommand{\Cn}[1]{\begin{center} #1 \end{center}}
\newcommand{\mpg}[2][1.0\hsize]{\begin{minipage}[b]{#1}{#2}\end{minipage}}
\newcommand{\mpgt}[2][1.0\hsize]{\begin{minipage}[t]{#1}{#2}\end{minipage}}

\newcommand{\rmrk}[1]{\textcolor{red}{#1}}
\newcommand{\Rmrk}[1]{\textcolor{blue}{\LARGE\bf #1}}
\newcommand{\hide}[1]{\rmrk{[hidden text]}}

% extra math commands by jarondl
%\newcommand{\aket}[1]{\left| #1 \right\rangle}
%\newcommand{\abra}[1]{\left\langle #1 \right|}
%\newcommand{\abraket}[2]{\left\langle #1 | #2   \right\rangle}
%\newcommand{\sandwich}[3]{\left\langle #1 | #2 | #3  \right\rangle}
%\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\inner}[2]{\left \langle #1 \middle| #2\right\rangle} % Inner product

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Notes}

\author{Yaron de Leeuw, Doron Cohen}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The mathematical properties of random matrices may offer interesting insight for the physical properties. Such physical properties include the diffusion and the Survival Probability, while the mathematical properties include the sparsity of the matrix and its spectrum. 

We shall consentrate on sparse matrices with log-normally distributed values. At first, we will work on a Nearest-Neighbor model as described in section \ref{sec:our_model}. One can think of this model as a one dimensional chain of sites, where transition rates between the sites vary log-normally.


Another method we apply is the SLRT - Semi Linear Response Theorem \cite{Stotland:2010:PRB}. Which maps the transition rate problem to a resistor network one. A known solution for the diffusion constant in a nearest neighbor network is \cite{Derrida:1983}:
\begin{align}
D=\overline{w}_{harmonic} =(\overline{w^{-1}})^{-1}=\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}


The works of Stotland \cite{Stotland:2010:PRB} \cite{Stotland:2009:EPL}, describe semi linear response by the use of sparse matrices. They focus mainly on the rate of energy absorption.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Master equation}
The equation governing the time developement in a transition rate problem is:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t)
\end{align}
Where $P_n(t) \ge 0$ is the probability to be at site $n$ at time $t$ and $W$ is the transition matrix.

The matrix has eigenmodes $V_\lambda$ with eigenvalues $\lambda$. 
We define a standard inner product :
\begin{align}
\inner{A}{B} = \sum_{ii} A_iB_i
\end{align}
Then we can write the probability vector $P$ as a sum of the normalized eigenmodes.
\begin{align}
P_n(t) = \sum_\lambda \inner{P(t)}{V_\lambda}{V_\lambda }_n = \sum_\lambda C_\lambda(t) {V_\lambda}_n
\end{align}
Where we have defined $C_\lambda(t)\equiv \inner{P(t)}{V_\lambda}$.


The master equation becomes
\begin{align}
&\sum_\lambda \left( \frac{dC_\lambda(t)}{dt}{V_\lambda}_n - C_\lambda(t)\sum_m W_{nm}{V_\lambda}_n \right)= \sum_\lambda \left( \frac{dC_\lambda(t)}{dt} - \lambda C_\lambda(t)\right){V_\lambda}_n =0\\
&C_\lambda(t) = C_\lambda(0)e^{\lambda t}\\
&P_n(t) = \sum_\lambda C_\lambda(t) {V_\lambda}_n = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_n 
\end{align}
The survival probability is defined as the probability to remain in a specific site. If we begin with a probabilty vector whose only non zero element is $P_i(0)=1$, the corresponding $C_\lambda(0)$s will be:
\begin{align}
C_\lambda(0) = \inner{P(0)}{V_\lambda}= {V_\lambda}_i
\end{align}
And then the survival probability will be:
\begin{align}
\mathcal{P}_i(t) = P_i(t) = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_i=\sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t} 
\end{align}
The mean survival probability will be:
\begin{align}
\mathcal{P}(t) = \sum_i \frac{1}{N} P_i(t)=\sum_i \frac{1}{N} \sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t}  =\frac{1}{N}\sum_\lambda e^{\lambda t}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Eigenvalue distribution}
In the supplementary material of \cite{Amir:2010:PRL} the moments of the distribution of the eigenvalues are found. 

If we define $D$ as the diagonalized matrix $W$ as in $W=UDU^{-1}$, we have
\begin{align}
\sum_\lambda \lambda^k = Tr (D^k) = Tr(UDU^{-1}UDU^{-1}UDU^{-1}UDU^{-1}...) = Tr (W^k)
\end{align}
So the exponent $k$ of the matrix $W$ is equal to the $k^{th}$ moment of the distribution of the eigenvalues.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Model}\label{sec:our_model}
The model consists of a chain of connected sites. Our point of intereset are the transition rates between those sites. One can define a master equation:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t) \\
W_{n,n-1} = W_{n-1,n} = w_n \\
W_{nn} = -w_n - w_{n+1}
\end{align}
Where the $w_n$s are the individual transition rates.

Less abstractly, the transition matrix is:
\begin{align}
W = 
\begin{pmatrix}
-w_1  & w_1 \\
w_1  & -w_1-w_2 &  w_2 \\
 & w_2 & -w_2-w_3 &  w_3 \\
& & w_3 & -w_3-w_4 & \; w_4 \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
%%%%%%%
\subsection{Log-normal}
The different $w_n$s are chosen from a log-normal distribution (by a special construction, see section \ref{sec:matrix_construction})

The harmonic mean of the log normal distribution with normal width $\sigma$ is given by:
\begin{align}
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}} \frac{1}{e^x} dx  = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}-x} dx \\
&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(x-\sigma^2\right)^2}{2\sigma^2}+\frac{\sigma^2}{2}} dx = e^{\frac{\sigma^2}{2}}\\
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = e^{-\frac{\sigma^2}{2}}
\end{align}
The generalization for $\mu\neq 0$ is :
\begin{align}
\overline{w}_{harmonic} = e^{-\frac{\sigma^2}{2}+\mu}
\end{align}
%%%%%
\subsection{Alternating transition rates}
We can think of a model where every second transition is different, so the transitions are a,b,a,b,a,b... And the matrix is:
\begin{align}
W = 
\begin{pmatrix}
-a  & a \\
a  & -a-b &  b \\
 & b & -a-b &  a \\
& & a & -a-b & \; b \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
This gives us two inidical equations, for even and odd $n$'s. If we take an odd $n$ the equations are:
\begin{align}
a {V_\lambda}_n - (a+b+\lambda){V_\lambda}_{n+1} + b {V_\lambda}_{n+2} =0  \\
b {V_\lambda}_{n+1} - (a+b+\lambda){V_\lambda}_{n+2} + a {V_\lambda}_{n+3} =0 
\end{align}
\begin{comment}    The eigenmodes have to be real... so this treatment is false, although the eigenmodes seem to be correct
The eigenvalues for this model can be found analytically, using bloch's theorem. The solution for a periodic structure with a unit cell is ${V_\lambda}_n = e^{ikn} u(x)$ where $u(x)$ is a function with the periodicity of the unit cell. In our case this simplifies to:
\begin{align}
{V_\lambda}_n = \begin{cases}e^{ikn} c_1 \; &, \text{n odd} \\ e^{ikn} c_2 \; &, \text{n even} \end{cases}
\end{align}
When we apply the eignenvalue equation to this vector ($WV_\lambda = \lambda V_\lambda$), we obtain two linear equations:
\begin{align}
ac_1e^{-ik}   -(a+b)c_2 +bc_1e^{ik} &= \lambda c_2 \\
bc_2e^{-ik}   -(a+b)c_1 +ac_2e^{ik} &= \lambda c_1 
\end{align}
The solution for $\lambda$ is
\begin{align}
(a+b+\lambda)^2 = a^2+b^2+2ab\cos 2k \\
\lambda = -(a + b) \pm \sqrt{a^2+b^2+2ab\cos 2k}
\end{align}
\end{comment}
\subsection{Log normal, but with less nodes}
Perhaps a model similar to the original model but with a small finite number of nodes is easier to solve analytically.
\subsection{Euclidean distance rates, or functions of it}
Random Euclidean distance matrices were widely researched \cite{Mezard:1999:NPB}, [insert more refs..]. Amir et al \cite{Amir:2010:PRL} have studied a model with the exponents of euclidean distances. 

In one dimension, the transition rates are related to the distance between points distributed uniformly. The nearest neighbor distance can thought of as a "Poisson Process" and then the distance is distributed exponentialy.


$\inner{1}{2}$

\begin{comment}
\section{unordered notes}
Some random unordered notes.
\begin{align}\label{eq:rho_of_t}
\dot{\aket{\rho}}&=W\aket{\rho} \\
\aket{\rho(t)} &= e^{Wt}\aket{\rho_0} \\
\aket{\rho(t)} &= \sum_i e^{Wt}\aket{i}\abraket{i}{\rho_0} \\
&= \sum_i e^{\lambda_i t}\aket{i}\abraket{i}{\rho_0}
\end{align}
The survival probability is averaging over the exponent of the Hamiltonian:
\begin{align}
\abraket{\rho_0}{\rho} &= \abra{\rho_0}e^{Wt}\aket{\rho_0} \\
&= \sum_i \abs{\abraket{i}{\rho_0}}^2e^{\lambda_it}
\end{align}
If $\aket{\rho_0}$ is defined as a specific site $\aket{n}$, one can average over all the sites to obtain the average survival probability per site $P(t)$: 
\begin{align}
\avg{ P(t) } &= \sum_n\sum_i \frac{1}{N}\abs{\abraket{i}{n}}^2e^{\lambda_it}= \sum_i e^{\lambda_i t}
\end{align}
\end{comment}

\section{Random Matrix Construction}\label{sec:matrix_construction}
To create a $N\times N$ matrix, we have to choose $N-1$ random numbers. To make sure our distribution is truly log-normal, we create $N-1$ numbers forming a log-normal distribution, and permute them. In order to create the numbers, first we create a vector of $N-1$ linearly spaced values between $0$ and $1$. Then we apply element-wise the inverse cumulative distribution function of the normal distribution \ref{eq:invcdf}, using the inverse complementary error function. This gives a $N-1$ vector with normal distributed values. The vector is then scaled ($\tilde{y} = \mu+\sigma\cdot y$) to the appropriate width and mean [of the normal distribution]. The last step is to permute the vector, and apply an exponent element-wise.
\begin{align}\label{eq:invcdf}
y = -\sqrt{2}\cdot\operatorname{erfcinv}(2\cdot x) 
\end{align}

\section{graphs}
I have created a random matrix as prescribed in section \ref{sec:matrix_construction}, with size $N=200$. With an initial state of $\rho_0=n=100$, several calculations have been made by matrix exponentiation as in eq. \ref{eq:rho_of_t}. A movie of rho as a function of t for $\sigma=1$ was produced ("rho.mpg"). Now, I have taken the initial transition rates, and rescaled them to recieve results with different $\sigma$'s. The rescaling was done by taking a power sigma of the initial values. ($e^{\sigma x} = {(e^x)}^\sigma$). Both plots are at t=500 for different $\sigma$'s. The first plot is of the second moment (where the 200 nodes are spread equally over the range (-1,1), while the second plot is of the survival.
\bibliographystyle{plain}
\bibliography{jarondl}
\end{document}
