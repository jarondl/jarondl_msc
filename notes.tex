%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[onecolumn,fleqn]{revtex4}


% special 
\usepackage{ifthen}
\usepackage{ifpdf}
\usepackage{float}
\usepackage{color}

% fonts
\usepackage{latexsym}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{bm}
\usepackage{wasysym}


\ifpdf
\usepackage{graphicx}
\usepackage{epstopdf}
\else
\usepackage{graphicx}
\usepackage{epsfig}
\fi

%by jarondl
\usepackage{verbatim} % for multiline comments
\usepackage{natbib}
\usepackage{hyperref}  % for hyperlinks in biblio. should be called last?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% NEW 
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\varphiJ}{\bm{\varphi}}
\newcommand{\thetaJ}{\bm{\theta}}
%\renewcommand{\includegraphics}[2][0]{FIGURE}


% math symbols I
\newcommand{\sinc}{\mbox{sinc}}
\newcommand{\const}{\mbox{const}}
\newcommand{\trc}{\mbox{trace}}
\newcommand{\intt}{\int\!\!\!\!\int }
\newcommand{\ointt}{\int\!\!\!\!\int\!\!\!\!\!\circ\ }
\newcommand{\ar}{\mathsf r}
\newcommand{\im}{\mbox{Im}}
\newcommand{\re}{\mbox{Re}}

% math symbols II
\newcommand{\eexp}{\mbox{e}^}
\newcommand{\bra}{\left\langle}
\newcommand{\ket}{\right\rangle}

% Mass symbol
\newcommand{\mass}{\mathsf{m}} 
\newcommand{\rdisc}{\epsilon} 

% more math commands
\newcommand{\tbox}[1]{\mbox{\tiny #1}}
\newcommand{\bmsf}[1]{\bm{\mathsf{#1}}} 
\newcommand{\amatrix}[1]{\begin{matrix} #1 \end{matrix}} 
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}

% equations
\newcommand{\be}[1]{\begin{eqnarray}\ifthenelse{#1=-1}{\nonumber}{\ifthenelse{#1=0}{}{\label{e#1}}}}
\newcommand{\ee}{\end{eqnarray}} 
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}} 



% arrangement
\newcommand{\drawline}{\begin{picture}(500,1)\line(1,0){500}\end{picture}}
\newcommand{\bitem}{$\bullet$ \ \ \ }
\newcommand{\Cn}[1]{\begin{center} #1 \end{center}}
\newcommand{\mpg}[2][1.0\hsize]{\begin{minipage}[b]{#1}{#2}\end{minipage}}
\newcommand{\mpgt}[2][1.0\hsize]{\begin{minipage}[t]{#1}{#2}\end{minipage}}

\newcommand{\rmrk}[1]{\textcolor{red}{#1}}
\newcommand{\Rmrk}[1]{\textcolor{blue}{\LARGE\bf #1}}
\newcommand{\hide}[1]{\rmrk{[hidden text]}}

% extra math commands by jarondl
%\newcommand{\aket}[1]{\left| #1 \right\rangle}
%\newcommand{\abra}[1]{\left\langle #1 \right|}
%\newcommand{\abraket}[2]{\left\langle #1 | #2   \right\rangle}
%\newcommand{\sandwich}[3]{\left\langle #1 | #2 | #3  \right\rangle}
%\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand{\inner}[2]{\left \langle #1 \middle| #2\right\rangle} % Inner product

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Notes}

\author{Yaron de Leeuw, Doron Cohen}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
The mathematical properties of random matrices may offer interesting insight for the physical properties. Such physical properties include the diffusion and the Survival Probability, while the mathematical properties include the sparsity of the matrix and its spectrum. 

We shall concentrate on sparse matrices with log-normally distributed values. At first, we will work on a Nearest-Neighbor model as described in section \ref{sec:our_model}. One can think of this model as a one dimensional chain of sites, where transition rates between the sites vary log-normally.


Another method we apply is the SLRT - Semi Linear Response Theorem \cite{Stotland:2010:PRB}. Which maps the transition rate problem to a resistor network one. A known solution for the diffusion constant in a nearest neighbor network is \cite{Derrida:1983}:
\begin{align}
D=\overline{w}_{harmonic} =(\overline{w^{-1}})^{-1}=\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}


The works of Stotland \cite{Stotland:2010:PRB} \cite{Stotland:2009:EPL}, describe semi linear response by the use of sparse matrices. They focus mainly on the rate of energy absorption.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Master equation}
The equation governing the time developement in a transition rate problem is:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t)
\end{align}
Where $P_n(t) \ge 0$ is the probability to be at site $n$ at time $t$ and $W$ is the transition matrix.

The matrix has eigenmodes $V_\lambda$ with eigenvalues $\lambda$. 
We define a standard inner product :
\begin{align}
\inner{A}{B} = \sum_{ii} A_iB_i
\end{align}
Then we can write the probability vector $P$ as a sum of the normalized eigenmodes.
\begin{align}
P_n(t) = \sum_\lambda \inner{P(t)}{V_\lambda}{V_\lambda }_n = \sum_\lambda C_\lambda(t) {V_\lambda}_n
\end{align}
Where we have defined $C_\lambda(t)\equiv \inner{P(t)}{V_\lambda}$.


The master equation becomes
\begin{align}
&\sum_\lambda \left( \frac{dC_\lambda(t)}{dt}{V_\lambda}_n - C_\lambda(t)\sum_m W_{nm}{V_\lambda}_n \right)= \sum_\lambda \left( \frac{dC_\lambda(t)}{dt} - \lambda C_\lambda(t)\right){V_\lambda}_n =0\\
&C_\lambda(t) = C_\lambda(0)e^{\lambda t}\\
&P_n(t) = \sum_\lambda C_\lambda(t) {V_\lambda}_n = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_n 
\end{align}
%%%%%%%%%%%%%
\subsection{Survival probability}
The survival probability is defined as the probability to remain in a specific site. If we begin with a probabilty vector whose only non zero element is $P_i(0)=1$, the corresponding $C_\lambda(0)$s will be:
\begin{align}
C_\lambda(0) = \inner{P(0)}{V_\lambda}= {V_\lambda}_ito
\end{align}
And then the survival probability will be:
\begin{align}
\mathcal{P}_i(t) = P_i(t) = \sum_\lambda  C_\lambda(0)e^{\lambda t} {V_\lambda}_i=\sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t} 
\end{align}
The mean survival probability will be:
\begin{align} \label{eq:surv_eigenvalues}
\mathcal{P}(t) = \sum_i \frac{1}{N} P_i(t)=\sum_i \frac{1}{N} \sum_\lambda ({V_\lambda}_i)^2 e^{\lambda t}  =\frac{1}{N}\sum_\lambda e^{\lambda t}
\end{align}
%%%%%%%%%%%%%%%
\subsection{Spreading}
The spreading of the probabilities is defined as the second moment of the locations:
\begin{align}
S(t) = \sum_n n^2 P_n -\left(\sum_n n P_n\right)^2
\end{align}
Alexander \cite{Alexander:1981:RMP} has shown that if the average of inverse transition rates is bounded ($\left(\frac{1}{N}\sum\frac{1}{w_n}\right) < \infty$) then the a spreading at $t\rightarrow \infty$ behaves like in an ordered chain:
\begin{align}
S(t) \approx 2Dt 
\end{align}
With a diffusion coefficient 
\begin{align}\label{eq:diff}
D = \left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1}
\end{align}

%%%%%%%%%%%%%%
\subsection{The relation between the spreading and the survival probability}
We have two known quantities, one is the survival probability $\mathcal{P}(t)$, which is obtained by the use of eigenmode decomposition, and the other is the spreading $S(t)$, which was derived from the harmonic mean. The relation between them is not always clear. 

The scaling hypothesis ??? shows that
\begin{align}
\mathcal{P}(t)\sqrt{S(t)}=1
\end{align}
On the other hand, we may be able to derive the spreading using the same methods we have used for the survival probability, i.e. eigenmode decomposition.

By definition:
\begin{align}
S(t) = \sum_n n^2 P_n -\left(\sum_n n P_n\right)^2 = \sum_n\sum_\lambda C_\lambda(0)e^{\lambda t}n^2 {V_\lambda}_n - (\sum_n\sum_\lambda C_\lambda(0)e^{\lambda t}n {V_\lambda}_n)^2
\end{align}
If we at start a specific site $i$, so that the only non zero probability is $P_i(0)=1$, the spreading will become:
\begin{align}
S_i(t) = \sum_n\sum_\lambda e^{\lambda t}n^2 {V_\lambda}_n{V_\lambda}_i - (\sum_n\sum_\lambda e^{\lambda t}n {V_\lambda}_n{V_\lambda}_i)^2
\end{align}
Averaging over the initial state $i$ we have:
\begin{align}
\overline{S}(t) = \frac{1}{N}\sum_i\sum_n\sum_\lambda e^{\lambda t}n^2 {V_\lambda}_n{V_\lambda}_i - \frac{1}{N}\sum_i(\sum_n\sum_\lambda e^{\lambda t}n {V_\lambda}_n{V_\lambda}_i)^2
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Eigenvalue distribution}
In the supplementary material of \cite{Amir:2010:PRL} the moments of the distribution of the eigenvalues are found. 

If we define $D$ as the diagonalized matrix $W$ as in $W=UDU^{-1}$, we have
\begin{align}\label{eq:eig_moments}
\sum_\lambda \lambda^k = Tr (D^k) = Tr(UDU^{-1}UDU^{-1}UDU^{-1}UDU^{-1}...) = Tr (W^k)
\end{align}
So the exponent $k$ of the matrix $W$ is equal to the $k^{th}$ moment of the distribution of the eigenvalues. If one can find the exponents of the matrix, he can find the moments of the distribution, and hopefuly the distribution itself. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Models}\label{sec:our_model}
The model consists of a chain of connected sites. Our point of intereset are the transition rates between those sites. One can define a master equation:
\begin{align}
\frac{dP_n(t)}{dt} = \sum_m W_{nm}P_m(t) \\
W_{n,n-1} = W_{n-1,n} = w_n \\
W_{nn} = -w_n - w_{n+1}
\end{align}
Where the $w_n$s are the individual transition rates.

Less abstractly, the transition matrix is:
\begin{align}
W = 
\begin{pmatrix}
-w_1  & w_1 \\
w_1  & -w_1-w_2 &  w_2 \\
 & w_2 & -w_2-w_3 &  w_3 \\
& & w_3 & -w_3-w_4 & \; w_4 \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
%%%%%%%
\subsection{Log-normal}
The different $w_n$s are chosen from a log-normal distribution (by a special construction, see section \ref{sec:matrix_construction})

The harmonic mean of the log normal distribution with normal width $\sigma$ is given by:
\begin{align}
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}} \frac{1}{e^x} dx  = \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}-x} dx \\
&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{\left(x-\sigma^2\right)^2}{2\sigma^2}+\frac{\sigma^2}{2}} dx = e^{\frac{\sigma^2}{2}}\\
\left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = e^{-\frac{\sigma^2}{2}}
\end{align}
The generalization for $\mu\neq 0$ is :
\begin{align}
\overline{w}_{harmonic} = e^{-\frac{\sigma^2}{2}+\mu}
\end{align}
%%%%%
\subsection{Alternating transition rates}
We can think of a model where every second transition is different, so the transitions are a,b,a,b,a,b... And the matrix is:
\begin{align}
W = 
\begin{pmatrix}
-a  & a \\
a  & -a-b &  b \\
 & b & -a-b &  a \\
& & a & -a-b & \; b \\
& & & \ddots &\ddots&\;\;\ddots
\end{pmatrix}
\end{align}
The diffusion coefficient according to \ref{eq:diff} should be:
\begin{align}
D = \left(\frac{1}{N}\sum\frac{1}{w_n}\right)^{-1} = \frac{2}{\frac{1}{a} + \frac{1}{b}} = \frac{2ab}{a+b}
\end{align}

Applying the eigenvalue equation to this matrix gives us two inidical equations, for even and odd $n$'s. If we take an odd $n$ the equations are:
\begin{align}
a {V_\lambda}_n - (a+b+\lambda){V_\lambda}_{n+1} + b {V_\lambda}_{n+2} =0  \\
b {V_\lambda}_{n+1} - (a+b+\lambda){V_\lambda}_{n+2} + a {V_\lambda}_{n+3} =0 
\end{align}
The solution with real eigenmodes is rather cumbersome, but the eigenvalues found by any set of orthogonal eigenmodes have to be the same. So for purely analytical reasons we find the eigenvalues with complex eigenmodes, although they have no meaning in our model.


The eigenvalues for this matrix can be found analytically, using bloch's theorem. The solution for a periodic structure with a unit cell is ${V_\lambda}_n = e^{ikn} u(x)$ where $u(x)$ is a function with the periodicity of the unit cell. In our case this simplifies to:
\begin{align}
{V_\lambda}_n = \begin{cases}e^{ikn} c_1 \; &, \text{n odd} \\ e^{ikn} c_2 \; &, \text{n even} \end{cases}
\end{align}
When we apply the eignenvalue equation to this vector ($WV_\lambda = \lambda V_\lambda$), we obtain two linear equations:
\begin{align}
ac_1e^{-ik}   -(a+b)c_2 +bc_1e^{ik} &= \lambda c_2 \\
bc_2e^{-ik}   -(a+b)c_1 +ac_2e^{ik} &= \lambda c_1 
\end{align}
The solution for $\lambda$ is
\begin{align}
(a+b+\lambda)^2 = a^2+b^2+2ab\cos 2k \\
\lambda = -(a + b) \pm \sqrt{a^2+b^2+2ab\cos 2k}  \label{eq:abab_eigenvals}
\end{align}
Now that we have the eigenvalues, we may use equation \ref{eq:surv_eigenvalues} to find the survival probability as a function of time:
\begin{align}
\mathcal{P}(t) =\frac{1}{N}\sum_\lambda e^{\lambda t} 
\end{align}

\subsection{Log normal, but with less nodes}
Perhaps a model similar to the original model but with a small finite number of nodes is easier to solve analytically.
\subsection{Euclidean distance rates, or functions of it}
Random Euclidean distance matrices were widely researched \cite{Mezard:1999:NPB}, [insert more refs..]. Amir et al \cite{Amir:2010:PRL} have studied a model with the exponents of euclidean distances. They obtain the survival probability by finding the distribution of the eigenvalues, using the idea of \ref{eq:eig_moments}. 

In one dimension, the transition rates are related to the distance between points distributed uniformly. The nearest neighbor distance can thought of as a "Poisson Process" and then the distance is distributed exponentialy.

The exponential distribution does not fit case $(a)$ of Alexander's review \cite{Alexander:1981:RMP}, so the diffusion coefficient cannot be derived with the same method.


\section{Random Matrix Construction}\label{sec:matrix_construction}
To create a $N\times N$ matrix, we have to choose $N-1$ random numbers. To make sure our distribution is truly log-normal, we create $N-1$ numbers forming a log-normal distribution, and permute them. In order to create the numbers, first we create a vector of $N-1$ linearly spaced values between $0$ and $1$. Then we apply element-wise the inverse cumulative distribution function of the normal distribution \ref{eq:invcdf}, using the inverse complementary error function. This gives a $N-1$ vector with normal distributed values. The vector is then scaled ($\tilde{y} = \mu+\sigma\cdot y$) to the appropriate width and mean [of the normal distribution]. The last step is to permute the vector, and apply an exponent element-wise.
\begin{align}\label{eq:invcdf}
y = -\sqrt{2}\cdot\operatorname{erfcinv}(2\cdot x) 
\end{align}

\bibliographystyle{plainnat}
\bibliography{jarondl}
\end{document}
